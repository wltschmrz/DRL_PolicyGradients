{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzpYjqmZU3lf",
        "outputId": "b1045a88-ef9b-4e06-cff6-b2c6148e6393"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[mujoco]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.12.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[mujoco])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.31.6)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
            "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.1)\n",
            "Installing collected packages: glfw, farama-notifications, gymnasium, mujoco\n",
            "Successfully installed farama-notifications-0.0.4 glfw-2.7.0 gymnasium-0.29.1 mujoco-3.1.6\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U7UQ5GajUueB"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import MultivariateNormal\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc_mean = nn.Linear(64, action_dim)\n",
        "        self.fc_log_std = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.fc_mean(x)\n",
        "        log_std = self.fc_log_std(x)\n",
        "        return mean, log_std"
      ],
      "metadata": {
        "id": "Zv-aRyevU51p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가우시안 분포 생성 함수\n",
        "def get_action_and_log_prob(state, policy):\n",
        "    mean, log_std = policy(state)\n",
        "    std = log_std.exp()\n",
        "    dist = MultivariateNormal(mean, torch.diag_embed(std))\n",
        "    action = dist.sample()\n",
        "    log_prob = dist.log_prob(action)\n",
        "    return action, log_prob"
      ],
      "metadata": {
        "id": "YLGz02dNVBvN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO 업데이트 함수\n",
        "def ppo_update(policy, optimizer, states, actions, rewards, old_log_probs, advantages):\n",
        "    for _ in range(K_epochs):\n",
        "        mean, log_std = policy(states)\n",
        "        std = log_std.exp()\n",
        "        dist = MultivariateNormal(mean, torch.diag_embed(std))\n",
        "        new_log_probs = dist.log_prob(actions)\n",
        "        ratio = (new_log_probs - old_log_probs).exp()\n",
        "\n",
        "        surrogate1 = ratio * advantages\n",
        "        surrogate2 = torch.clamp(ratio, 1 - epsilon_clip, 1 + epsilon_clip) * advantages\n",
        "\n",
        "        loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "SB2QNr59VBxj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "env = gym.make('Ant-v4')\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]"
      ],
      "metadata": {
        "id": "TxBy3ARhU539"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "learning_rate = 3e-4\n",
        "gamma = 0.99\n",
        "epsilon_clip = 0.2\n",
        "K_epochs = 10\n",
        "T_horizon = 2048"
      ],
      "metadata": {
        "id": "-mdAZl4GU5zZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화 및 옵티마이저 설정\n",
        "policy = PolicyNetwork()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "6AJPexrvU5pr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메인 학습 루프\n",
        "for episode in range(1000):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    rewards = []\n",
        "    log_probs = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(T_horizon):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        action, log_prob = get_action_and_log_prob(state, policy)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action.detach().numpy())\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Advantage 계산\n",
        "    discounted_rewards = []\n",
        "    for t in range(len(rewards)):\n",
        "        Gt = sum([gamma ** i * rewards[t + i] for i in range(len(rewards) - t)])\n",
        "        discounted_rewards.append(Gt)\n",
        "\n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    states = torch.stack(states)\n",
        "    actions = torch.stack(actions)\n",
        "    old_log_probs = torch.stack(log_probs).detach()\n",
        "\n",
        "    advantages = discounted_rewards - discounted_rewards.mean()\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
        "\n",
        "    # PPO 업데이트\n",
        "    ppo_update(policy, optimizer, states, actions, discounted_rewards, old_log_probs, advantages)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}: Reward {episode_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUCQt5m0VCYl",
        "outputId": "e24e0fa4-c9a2-41b7-c73d-43b02d12f14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Reward -7729.14562595329\n",
            "Episode 10: Reward -4863.71556950801\n",
            "Episode 20: Reward -3000.679376661323\n",
            "Episode 30: Reward -1741.388917285942\n",
            "Episode 40: Reward -481.0536939356044\n",
            "Episode 50: Reward 227.70838585125733\n",
            "Episode 60: Reward 523.2066076521032\n",
            "Episode 70: Reward 858.425029763921\n",
            "Episode 80: Reward 1050.723646972466\n",
            "Episode 90: Reward 1073.9205313406796\n",
            "Episode 100: Reward 1178.3620112389242\n",
            "Episode 110: Reward 1183.904337140508\n",
            "Episode 120: Reward 1235.3981510830054\n",
            "Episode 130: Reward 1448.1357556664525\n",
            "Episode 140: Reward 1385.4940454702562\n",
            "Episode 150: Reward 1354.2113396554357\n",
            "Episode 160: Reward 1329.6911948224163\n",
            "Episode 170: Reward 1434.185604585034\n",
            "Episode 180: Reward 1477.6207080721774\n",
            "Episode 190: Reward 1435.3814802324823\n",
            "Episode 200: Reward 1453.7460626010743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-oBlh-Y8VCdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnfDLqs9VCgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yEYnurWVCh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}